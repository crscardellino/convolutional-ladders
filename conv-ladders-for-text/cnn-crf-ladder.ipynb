{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import input_data_crf\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "config = {\n",
    "   \"num_classes\" : 6,\n",
    "   \"denoising_cost\" : [1000, 10, 0.01],\n",
    "   \"noise_std\" : 0.3,\n",
    "   \"decay_after\" : 5,\n",
    "   \"experiment_id\" : \"0e0e6bea5b6a02b12e7767395a81bcf0\",\n",
    "   \"num_labeled\" : 100000,\n",
    "   \"embeddings_size\" : 128,\n",
    "   \"batch_size\" : 2048,\n",
    "   \"num_epochs\" : 5,\n",
    "   \"max_sentence_len\" : 20,\n",
    "   \"max_word_len\": 15,\n",
    "   \"starter_learning_rate\" : 0.02,\n",
    "   \"vocab_size\" : 74832,\n",
    "   \"conv_kernels\": [2,3,4,5],\n",
    "   \"conv_filters\": 32,\n",
    "   \"train_embeddings\": false,\n",
    "   \"lambda\": 0.1,\n",
    "   \"dropout\": 0.5,\n",
    "   \"embeddings\": \"/newhome/ccardellino/mirel/data/wiki_embeddings.npz\"\n",
    "}\n",
    "\n",
    "data = input_data_crf.read_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "# Previous operations\n",
    "####################################################################################\n",
    "conv_kernels = config['conv_kernels']\n",
    "conv_filters = config['conv_filters']\n",
    "num_classes = config[\"num_classes\"]\n",
    "\n",
    "tf.reset_default_graph()  # Clear the tensorflow graph (free reserved memory)\n",
    "\n",
    "####################################################################################\n",
    "# Inputs setup\n",
    "####################################################################################\n",
    "max_sentence_len = config['max_sentence_len']\n",
    "max_word_len = config['max_word_len']\n",
    "\n",
    "# feedforward_inputs (FFI): inputs for the feedforward network (i.e. the encoder).\n",
    "# Should contain the labeled training data (padded to max_sentence_len).\n",
    "feedforward_inputs = tf.placeholder(tf.int32,\n",
    "                                    shape=(None, max_sentence_len, max_word_len),\n",
    "                                    name=\"FFI\")\n",
    "\n",
    "# autoencoder_inputs (AEI): inputs for the autoencoder (encoder + decoder).\n",
    "# Should contain the unlabeled training data (also padded to max_sentence_len).\n",
    "autoencoder_inputs = tf.placeholder(tf.int32,\n",
    "                                    shape=(None, max_sentence_len, max_word_len),\n",
    "                                    name=\"AEI\")\n",
    "\n",
    "outputs = tf.placeholder(tf.int32, shape=(None, max_sentence_len))  # target\n",
    "sequences_lengths = tf.placeholder(tf.int32, shape=(None,))\n",
    "training = tf.placeholder(tf.bool)  # training or evaluation\n",
    "\n",
    "# Not quite sure what is this for\n",
    "FFI = tf.reshape(feedforward_inputs, [-1] + [max_sentence_len])\n",
    "AEI = tf.reshape(autoencoder_inputs, [-1] + [max_sentence_len])\n",
    "\n",
    "####################################################################################\n",
    "# Embeddings weights\n",
    "####################################################################################\n",
    "embeddings_size = config['embeddings_size']\n",
    "vocab_size = config['vocab_size']\n",
    "\n",
    "# Loading\n",
    "embeddings_weights = np.load(config[\"embeddings\"])[\"embeddings\"]\n",
    "embeddings_weights = tf.get_variable(\"embeddings\",\n",
    "                                     embeddings_weights.shape,\n",
    "                                     initializer=tf.constant_initializer(embeddings_weights),\n",
    "                                     trainable=config.get(\"train_embeddings\", False))\n",
    "FFI_embeddings = tf.expand_dims(\n",
    "    tf.nn.embedding_lookup(embeddings_weights, FFI),\n",
    "    axis=-1,\n",
    "    name=\"FFI_embeddings\")\n",
    "AEI_embeddings = tf.expand_dims(\n",
    "    tf.nn.embedding_lookup(embeddings_weights, AEI),\n",
    "    axis=-1,\n",
    "    name=\"AEI_embeddings\")\n",
    "\n",
    "####################################################################################\n",
    "# Batch normalization setup & functions\n",
    "####################################################################################\n",
    "# to calculate the moving averages of mean and variance\n",
    "ewma = tf.train.ExponentialMovingAverage(decay=0.99)\n",
    "# this list stores the updates to be made to average mean and variance\n",
    "bn_assigns = []\n",
    "\n",
    "def update_batch_normalization(batch, output_name=\"bn\", scope_name=\"BN\"):\n",
    "    dim = len(batch.get_shape().as_list())\n",
    "    mean, var = tf.nn.moments(batch, axes=list(range(0, dim-1)))\n",
    "    # Function to be used during the learning phase.\n",
    "    # Normalize the batch and update running mean and variance.\n",
    "    with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE):\n",
    "        running_mean = tf.get_variable(\"running_mean\",\n",
    "                                       mean.shape,\n",
    "                                       initializer=tf.constant_initializer(0))\n",
    "        running_var = tf.get_variable(\"running_var\",\n",
    "                                      mean.shape,\n",
    "                                      initializer=tf.constant_initializer(1))\n",
    "\n",
    "    assign_mean = running_mean.assign(mean)\n",
    "    assign_var = running_var.assign(var)\n",
    "    bn_assigns.append(ewma.apply([running_mean, running_var]))\n",
    "\n",
    "    with tf.control_dependencies([assign_mean, assign_var]):\n",
    "        z = (batch - mean) / tf.sqrt(var + 1e-10)\n",
    "        return tf.identity(z, name=output_name)\n",
    "\n",
    "def batch_normalization(batch, mean=None, var=None, output_name=\"bn\"):\n",
    "    if mean is None or var is None:\n",
    "        dim = len(batch.get_shape().as_list())\n",
    "        mean, var = tf.nn.moments(batch, axes=list(range(0, dim-1)))\n",
    "    z = (batch - mean) / tf.sqrt(var + tf.constant(1e-10))\n",
    "    return tf.identity(z, name=output_name)\n",
    "\n",
    "####################################################################################\n",
    "# Encoder\n",
    "####################################################################################\n",
    "def encoder_layer(z_pre, noise_std, update_BN, activation):\n",
    "    # Compute mean and variance of z_pre (to be used in the decoder)\n",
    "    dim = len(z_pre.get_shape().as_list())\n",
    "    mean, var = tf.nn.moments(z_pre, axes=list(range(0, dim-1)))\n",
    "    # Create a variable to store the values for latter retrieving them\n",
    "    _ = tf.identity(mean, name=\"mean\"), tf.identity(var, name=\"var\")\n",
    "\n",
    "    # Batch normalization\n",
    "    def training_batch_norm():\n",
    "        if update_BN:\n",
    "            z = update_batch_normalization(z_pre)\n",
    "        else:\n",
    "            z = batch_normalization(z_pre)\n",
    "\n",
    "        return z\n",
    "\n",
    "    def eval_batch_norm():\n",
    "        with tf.variable_scope(\"BN\", reuse=tf.AUTO_REUSE):\n",
    "            mean = ewma.average(tf.get_variable(\"running_mean\",\n",
    "                                                shape=z_pre.shape[-1]))\n",
    "            var = ewma.average(tf.get_variable(\"running_var\",\n",
    "                                               shape=z_pre.shape[-1]))\n",
    "        z = batch_normalization(z_pre, mean, var)\n",
    "        return z\n",
    "\n",
    "    # Perform batch norm depending to the phase (training or testing)\n",
    "    z = tf.cond(training, training_batch_norm, eval_batch_norm)\n",
    "    z += tf.random_normal(tf.shape(z)) * noise_std\n",
    "    z = tf.identity(z, name=\"z\")\n",
    "\n",
    "    # Center and scale plus activation\n",
    "    size = z.get_shape().as_list()[-1]\n",
    "    beta = tf.get_variable(\"beta\", [size],\n",
    "                           initializer=tf.constant_initializer(0))\n",
    "    gamma = tf.get_variable(\"gamma\", [size],\n",
    "                            initializer=tf.constant_initializer(1))\n",
    "\n",
    "    h = activation(z*gamma + beta)\n",
    "    return tf.identity(h, name=\"h\")\n",
    "\n",
    "def encoder(x, noise_std, update_BN):\n",
    "    # Perform encoding for each layer\n",
    "    x += tf.random_normal(tf.shape(x)) * noise_std\n",
    "    x = tf.identity(x, \"h0\")\n",
    "\n",
    "    # Build the \"wide\" convolutional layer for each conv_kernel\n",
    "    # This is the \"first\" layer\n",
    "    conv_features = []\n",
    "    weight_variables = []\n",
    "    for i, ksize in enumerate(conv_kernels, start=1):\n",
    "        with tf.variable_scope(\"encoder_bloc_\" + str(i), reuse=tf.AUTO_REUSE):\n",
    "            W = tf.get_variable(\"W\",\n",
    "                    (ksize, embeddings_size, 1, conv_filters),\n",
    "                    initializer=tf.truncated_normal_initializer())\n",
    "            weight_variables.append(W)\n",
    "            z_pre = tf.nn.conv2d(x, W, strides=[1,1,1,1],\n",
    "                    padding=\"VALID\", name=\"z_pre\")\n",
    "            h = encoder_layer(z_pre, noise_std, update_BN=update_BN,\n",
    "                              activation=tf.nn.relu)\n",
    "            h = tf.nn.max_pool(h,\n",
    "                    ksize=[1, max_sentence_len - ksize + 1, 1, 1],\n",
    "                    strides=[1,1,1,1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"global_max_pool\")\n",
    "            conv_features.append(h)\n",
    "\n",
    "    # Build the features layer (\"second\" layer)\n",
    "    total_kernels = len(conv_kernels)\n",
    "    total_conv_features = total_kernels * conv_filters\n",
    "    with tf.variable_scope(\"encoder_bloc_\" + str(total_kernels+1), reuse=tf.AUTO_REUSE):\n",
    "        h = tf.concat(conv_features, 3)\n",
    "        h = tf.reshape(h, (-1, total_conv_features), name=\"h\")\n",
    "\n",
    "    # Build the features to classes layer (\"last\" layer)\n",
    "    with tf.variable_scope(\"encoder_bloc_\" + str(total_kernels+2), reuse=tf.AUTO_REUSE):\n",
    "        W = tf.get_variable(\"W\", (total_conv_features, num_classes),\n",
    "                            initializer=tf.random_normal_initializer())\n",
    "        weight_variables.append(W)\n",
    "        z_pre = tf.matmul(h, W, name=\"z_pre\")\n",
    "        h = encoder_layer(z_pre, noise_std, update_BN=update_BN,\n",
    "                          activation=tf.nn.softmax)\n",
    "\n",
    "    y = tf.identity(h, name=\"y\")\n",
    "    return y, weight_variables\n",
    "\n",
    "noise_std = config['noise_std']\n",
    "\n",
    "with tf.name_scope(\"FF_clean\"):\n",
    "    # output of the clean encoder. Used for prediction\n",
    "    FF_y, weight_variables = encoder(FFI_embeddings, 0, update_BN=False)\n",
    "with tf.name_scope(\"FF_corrupted\"):\n",
    "    # output of the corrupted encoder. Used for training.\n",
    "    FF_y_corr, _ = encoder(FFI_embeddings, noise_std, update_BN=False)\n",
    "\n",
    "with tf.name_scope(\"AE_clean\"):\n",
    "    # corrupted encoding of unlabeled instances\n",
    "    AE_y, _ = encoder(AEI_embeddings, 0, update_BN=True)\n",
    "with tf.name_scope(\"AE_corrupted\"):\n",
    "    # corrupted encoding of unlabeled instances\n",
    "    AE_y_corr, _ = encoder(AEI_embeddings, noise_std, update_BN=False)\n",
    "\n",
    "l2_reg = tf.constant(0.0)\n",
    "for we_var in weight_variables:\n",
    "    l2_reg += tf.nn.l2_loss(we_var)\n",
    "\n",
    "####################################################################################\n",
    "# Decoder\n",
    "####################################################################################\n",
    "\n",
    "def g_gauss(z_c, u, output_name=\"z_est\", scope_name=\"denoising_func\"):\n",
    "    # gaussian denoising function proposed in the original paper\n",
    "    size = u.get_shape().as_list()[-1]\n",
    "\n",
    "    def wi(inits, name):\n",
    "        return tf.Variable(inits * tf.ones([size]), name=name)\n",
    "\n",
    "    with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE):\n",
    "        a1 = wi(0., 'a1')\n",
    "        a2 = wi(1., 'a2')\n",
    "        a3 = wi(0., 'a3')\n",
    "        a4 = wi(0., 'a4')\n",
    "        a5 = wi(0., 'a5')\n",
    "\n",
    "        a6 = wi(0., 'a6')\n",
    "        a7 = wi(1., 'a7')\n",
    "        a8 = wi(0., 'a8')\n",
    "        a9 = wi(0., 'a9')\n",
    "        a10 = wi(0., 'a10')\n",
    "\n",
    "        mu = a1 * tf.sigmoid(a2 * u + a3) + a4 * u + a5\n",
    "        v = a6 * tf.sigmoid(a7 * u + a8) + a9 * u + a10\n",
    "\n",
    "        z_est = (z_c - mu) * v + mu\n",
    "    return tf.identity(z_est, name=output_name)\n",
    "\n",
    "def get_tensor(input_name, num_encoder_bloc, name_tensor):\n",
    "    return tf.get_default_graph().\\\n",
    "        get_tensor_by_name(input_name + \"/encoder_bloc_\" +\n",
    "                           str(num_encoder_bloc) + \"/\" + name_tensor + \":0\")\n",
    "\n",
    "denoising_cost = config['denoising_cost']\n",
    "d_cost = []\n",
    "u = batch_normalization(AE_y_corr, output_name=\"u_L\")\n",
    "\n",
    "# Build first decoder layer (corresponding to the dense layer)\n",
    "total_kernels = len(conv_kernels)\n",
    "total_conv_features = total_kernels * conv_filters\n",
    "with tf.variable_scope(\"decoder_bloc_\" + str(total_kernels+2), reuse=tf.AUTO_REUSE):\n",
    "    z_corr = get_tensor(\"AE_corrupted\", total_kernels+2, \"z\")\n",
    "    z = get_tensor(\"AE_clean\", total_kernels+2, \"z\")\n",
    "    mean = get_tensor(\"AE_clean\", total_kernels+2, \"mean\")\n",
    "    var = get_tensor(\"AE_clean\", total_kernels+2, \"var\")\n",
    "    # Performs the decoding operations of a corresponding encoder bloc\n",
    "    # Denoising\n",
    "    z_est = g_gauss(z_corr, u)\n",
    "\n",
    "    z_est_BN = (z_est - mean)/tf.sqrt(var + tf.constant(1e-10))\n",
    "    z_est_BN = tf.identity(z_est_BN, name=\"z_est_BN\")\n",
    "\n",
    "    # run decoder layer\n",
    "    V = tf.get_variable(\"V\", (num_classes, total_conv_features),\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "    l2_reg += tf.nn.l2_loss(V)\n",
    "    u = tf.matmul(z_est, V)\n",
    "    u = batch_normalization(u, output_name=\"u\")\n",
    "\n",
    "    d_cost.append((tf.reduce_mean(tf.square(z_est_BN - z))) * denoising_cost[2])\n",
    "\n",
    "# Build second decoder layer (corresponding to the concatenation+flat layer)\n",
    "with tf.variable_scope(\"decoder_bloc_\" + str(total_kernels+1), reuse=tf.AUTO_REUSE):\n",
    "    u = tf.reshape(u, (-1, 1, 1, total_conv_features))\n",
    "    deconv_features = tf.split(u, total_kernels, axis=3)\n",
    "\n",
    "# Build the final \"wide convolutional\" layer\n",
    "deconv_layers = []\n",
    "for i, gmp_layer in enumerate(deconv_features, start=1):\n",
    "    ksize = conv_kernels[i-1]\n",
    "    with tf.variable_scope(\"decoder_bloc_\" + str(i), reuse=tf.AUTO_REUSE):\n",
    "        u = tf.keras.layers.UpSampling2D(\n",
    "                size=(max_sentence_len - ksize + 1, 1))(gmp_layer)\n",
    "\n",
    "        z_corr = get_tensor(\"AE_corrupted\", i, \"z\")\n",
    "        z = get_tensor(\"AE_clean\", i, \"z\")\n",
    "        mean = get_tensor(\"AE_clean\", i, \"mean\")\n",
    "        var = get_tensor(\"AE_clean\", i, \"var\")\n",
    "        z_est = g_gauss(z_corr, u)\n",
    "\n",
    "        z_est_BN = (z_est - mean)/tf.sqrt(var + tf.constant(1e-10))\n",
    "        z_est_BN = tf.identity(z_est_BN, name=\"z_est_BN\")\n",
    "\n",
    "        # run deconvolutional (transposed convolution) layer\n",
    "        V = tf.get_variable(\"V\",\n",
    "                (ksize, embeddings_size, 1, conv_filters),\n",
    "                initializer=tf.truncated_normal_initializer())\n",
    "        l2_reg += tf.nn.l2_loss(V)\n",
    "\n",
    "        u = tf.nn.conv2d_transpose(z_est, V,\n",
    "                output_shape=tf.shape(AEI_embeddings),\n",
    "                strides=[1,1,1,1], padding='VALID')\n",
    "        u = batch_normalization(u, output_name=\"u\")\n",
    "        deconv_layers.append(u)\n",
    "        d_cost.append((tf.reduce_mean(tf.square(z_est_BN - z))) * denoising_cost[1])\n",
    "\n",
    "# last decoding step\n",
    "u = tf.concat(deconv_layers, 2)\n",
    "with tf.variable_scope(\"decoder_bloc_0\", reuse=tf.AUTO_REUSE):\n",
    "    z_corr = tf.get_default_graph().get_tensor_by_name(\"AE_corrupted/h0:0\")\n",
    "    z_corr = tf.concat([z_corr] * total_kernels, 2)\n",
    "    z = tf.get_default_graph().get_tensor_by_name(\"AE_clean/h0:0\")\n",
    "    z = tf.concat([z] * total_kernels, 2)\n",
    "    z_est = g_gauss(z_corr, u)\n",
    "    d_cost.append((tf.reduce_mean(tf.square(z_est - z))) * denoising_cost[0])\n",
    "\n",
    "####################################################################################\n",
    "# Loss, accuracy and optimization\n",
    "####################################################################################\n",
    "\n",
    "u_cost = tf.add_n(d_cost)  # reconstruction cost\n",
    "\n",
    "crf_scores_corr = tf.reshape(FF_y_corr, [-1, max_sentence_len, num_classes])\n",
    "crf_scores_clean = tf.reshape(FF_y, [-1, max_sentence_len, num_classes])\n",
    "with tf.variable_scope(\"crf\", reuse=tf.AUTO_REUSE):\n",
    "    log_likelihood_corr, _ = tf.contrib.crf.crf_log_likelihood(crf_scores_corr,\n",
    "            outputs, sequences_lengths)\n",
    "    log_likelihood_clean, trp = tf.contrib.crf.crf_log_likelihood(crf_scores_clean,\n",
    "            outputs, sequences_lengths)\n",
    "corr_pred_cost = tf.reduce_mean(-log_likelihood_corr)\n",
    "clean_pred_cost = tf.reduce_mean(-log_likelihood_clean)\n",
    "\n",
    "viterbi_sequence, _ = tf.contrib.crf.crf_decode(crf_scores_clean, trp, sequences_lengths)\n",
    "\n",
    "loss = corr_pred_cost + u_cost + config.get(\"lambda\", 0.0) * l2_reg # total cost\n",
    "\n",
    "# predictions = tf.argmax(FF_y, 1)\n",
    "# correct_prediction = tf.equal(predictions, tf.argmax(outputs, 1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "def calculate_metrics(true, pred, sequence_lens, max_len=max_sentence_len):\n",
    "    mask = (np.expand_dims(np.arange(max_len), axis=0) < np.expand_dims(sequence_lens, axis=1))\n",
    "    true = true[mask]\n",
    "    pred = pred[mask]\n",
    "    total_labels = np.sum(sequence_lens)\n",
    "    correct_labels = np.sum(true == pred)\n",
    "    accuracy = correct_labels / float(total_labels)\n",
    "    return true, pred, accuracy\n",
    "\n",
    "# Optimization setting\n",
    "starter_learning_rate = config['starter_learning_rate']\n",
    "learning_rate = tf.Variable(starter_learning_rate, trainable=False)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# add the updates of batch normalization statistics to train_step\n",
    "bn_updates = tf.group(*bn_assigns)\n",
    "with tf.control_dependencies([train_step]):\n",
    "    train_step = tf.group(bn_updates)\n",
    "\n",
    "n = np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])\n",
    "print(\"There is a total of %d trainable parameters\" % n, file=sys.stderr)\n",
    "\n",
    "####################################################################################\n",
    "# Training\n",
    "####################################################################################\n",
    "print(\"===  Loading Data ===\", file=sys.stderr)\n",
    "data = input_data_crf.read_data_sets(data_path,\n",
    "                                     n_classes=config['num_classes'],\n",
    "                                     n_labeled=config['num_labeled'],\n",
    "                                     maxlen=max_sentence_len)\n",
    "num_examples = data.train.unlabeled_ds.instances.shape[0]\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "num_epochs = config['num_epochs']\n",
    "\n",
    "num_iter = (num_examples//batch_size) * num_epochs  # number of loop iterations\n",
    "\n",
    "print(\"===  Starting Session ===\", file=sys.stderr)\n",
    "dev_config = tf.ConfigProto()                                                \n",
    "# Don't pre-allocate memory; allocate as-needed                          \n",
    "dev_config.gpu_options.allow_growth = True                                   \n",
    "# Only allow a total of half the GPU memory to be allocated              \n",
    "dev_config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "sess = tf.Session(config=dev_config)\n",
    "results_log = open(results_file, \"w\")\n",
    "print(\"experiment,split,epoch,accuracy,lloss,true,pred\", file=results_log)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "print(\"=== Training Start ===\", file=sys.stderr)\n",
    "tr = trange(0, num_iter, desc=\"iter: nan - loss: nan\")\n",
    "for i in tr:\n",
    "    labeled_instances, labels, sequences, unlabeled_instances = data.train.next_batch(batch_size)\n",
    "\n",
    "    _, tloss, lloss = sess.run([train_step, loss, clean_pred_cost],\n",
    "            feed_dict={feedforward_inputs: labeled_instances,\n",
    "                outputs: labels,\n",
    "                autoencoder_inputs: unlabeled_instances,\n",
    "                sequences_lengths: sequences,\n",
    "                training: True})\n",
    "    tr.set_description(\"loss: %.5g - lloss: %.5g\" % (tloss, lloss))\n",
    "\n",
    "    if (i > 1) and ((i+1) % (num_iter/num_epochs) == 0) and i < num_iter-1:\n",
    "        # Compute train and validation stats for each epoch\n",
    "        epoch_n = i//(num_examples//batch_size) + 1\n",
    "\n",
    "        tqdm.write(\"=== Epoch %d stats ===\" % epoch_n, file=sys.stderr)\n",
    "        # For training data we traverse in batches and save all the information\n",
    "        training_instances = data.train.labeled_ds.instances\n",
    "        training_labels = data.train.labeled_ds.labels\n",
    "        training_sequences = data.train.labeled_ds.sequences\n",
    "        mean_accuracy = []\n",
    "        mean_loss = []\n",
    "\n",
    "        for start in trange(0, len(training_labels), batch_size):\n",
    "            end = min(start+batch_size, len(training_labels))\n",
    "            epoch_stats = sess.run(\n",
    "                [viterbi_sequence, clean_pred_cost],\n",
    "                feed_dict={feedforward_inputs: training_instances[start:end],\n",
    "                           outputs: training_labels[start:end],\n",
    "                           sequences_lengths: training_sequences[start:end],\n",
    "                           training: False})\n",
    "            true_labels, pred_labels, acc = calculate_metrics(training_labels[start:end],\n",
    "                    epoch_stats[0], training_sequences[start:end])\n",
    "\n",
    "            mean_accuracy.append(acc)\n",
    "            mean_loss.append(epoch_stats[1])\n",
    "\n",
    "            true_labels = np.argmax(training_labels[start:end], 1)\n",
    "            for i in np.arange(true_labels.shape[0]):\n",
    "                print(\"%s,training,%d,%.3g,%.3g,%d,%d\" %\n",
    "                      (config[\"experiment_id\"],\n",
    "                       epoch_n,\n",
    "                       acc,\n",
    "                       epoch_stats[1],\n",
    "                       true_labels[i],\n",
    "                       pred_labels[i]), file=results_log)\n",
    "\n",
    "        tqdm.write(\"Epoch %d: Accuracy for Training Data: %.3g\" %\n",
    "                   (epoch_n, np.mean(mean_accuracy)), file=sys.stderr)\n",
    "        tqdm.write(\"Epoch %d: Supervised Cost for Training Data: %.3g\" %\n",
    "                   (epoch_n, np.mean(mean_loss)), file=sys.stderr)\n",
    "\n",
    "        # For validation data we traverse in batches and save all the information\n",
    "        validation_instances = data.validation.instances\n",
    "        validation_labels = data.validation.labels\n",
    "        validation_sequences = data.validation.sequences\n",
    "        mean_accuracy = []\n",
    "        mean_loss = []\n",
    "\n",
    "        for start in trange(0, len(validation_labels), batch_size):\n",
    "            end = min(start+batch_size, len(validation_labels))\n",
    "            epoch_stats = sess.run(\n",
    "                [viterbi_sequence, clean_pred_cost],\n",
    "                feed_dict={feedforward_inputs: validation_instances[start:end],\n",
    "                           outputs: validation_labels[start:end],\n",
    "                           sequences_lengths: validation_sequences[start:end],\n",
    "                           training: False})\n",
    "            true_labels, pred_labels, acc = calculate_metrics(validation_labels[start:end],\n",
    "                    epoch_stats[0], validation_sequences[start:end])\n",
    "\n",
    "            mean_accuracy.append(acc)\n",
    "            mean_loss.append(epoch_stats[1])\n",
    "\n",
    "            true_labels = np.argmax(validation_labels[start:end], 1)\n",
    "            for i in np.arange(true_labels.shape[0]):\n",
    "                print(\"%s,validation,%d,%.3g,%.3g,%d,%d\" %\n",
    "                      (config[\"experiment_id\"],\n",
    "                       epoch_n,\n",
    "                       acc,\n",
    "                       epoch_stats[1],\n",
    "                       true_labels[i],\n",
    "                       pred_labels[i]), file=results_log)\n",
    "\n",
    "        tqdm.write(\"Epoch %d: Accuracy for Validation Data: %.3g\" %\n",
    "                   (epoch_n, np.mean(mean_accuracy)), file=sys.stderr)\n",
    "        tqdm.write(\"Epoch %d: Supervised Cost for Validation Data: %.3g\" %\n",
    "                   (epoch_n, np.mean(mean_loss)), file=sys.stderr)\n",
    "\n",
    "        results_log.flush()\n",
    "        sys.exit(1)\n",
    "\n",
    "        decay_after = config['decay_after']\n",
    "        if (epoch_n+1) >= decay_after:\n",
    "            # decay learning rate\n",
    "            # learning_rate = starter_learning_rate * ((num_epochs - epoch_n) / (num_epochs - decay_after))\n",
    "            ratio = 1.0 * (num_epochs - (epoch_n+1))  # epoch_n + 1 because learning rate is set for next epoch\n",
    "            ratio = max(0, ratio / (num_epochs - decay_after))\n",
    "            sess.run(learning_rate.assign(starter_learning_rate * ratio))\n",
    "\n",
    "print(\"=== Final stats ===\", file=sys.stderr)\n",
    "epoch_n = num_iter//(num_examples//batch_size) + 1\n",
    "\n",
    "training_instances = data.train.labeled_ds.instances\n",
    "training_labels = data.train.labeled_ds.labels\n",
    "mean_accuracy = []\n",
    "mean_loss = []\n",
    "\n",
    "for start in trange(0, len(training_labels), batch_size):\n",
    "    end = min(start+batch_size, len(training_labels))\n",
    "    final_stats = sess.run(\n",
    "        [accuracy, clean_pred_cost, predictions],\n",
    "        feed_dict={feedforward_inputs: training_instances[start:end],\n",
    "                   outputs: training_labels[start:end],\n",
    "                   training: False})\n",
    "\n",
    "    mean_accuracy.append(final_stats[0])\n",
    "    mean_loss.append(final_stats[1])\n",
    "\n",
    "    true_labels = np.argmax(training_labels[start:end], 1)\n",
    "    for i in np.arange(true_labels.shape[0]):\n",
    "        print(\"%s,training,%d,%.3g,%.3g,%d,%d\" %\n",
    "              (config[\"experiment_id\"],\n",
    "               epoch_n,\n",
    "               final_stats[0],\n",
    "               final_stats[1],\n",
    "               true_labels[i],\n",
    "               final_stats[2][i]), file=results_log)\n",
    "\n",
    "print(\"Final Accuracy for Training Data: %.3g\" % np.mean(mean_accuracy), file=sys.stderr)\n",
    "print(\"Final Supervised Cost for Training Data: %.3g\" % np.mean(mean_loss), file=sys.stderr)\n",
    "\n",
    "# For validation data we traverse in batches and save all the information\n",
    "validation_instances = data.validation.instances\n",
    "validation_labels = data.validation.labels\n",
    "mean_accuracy = []\n",
    "mean_loss = []\n",
    "\n",
    "for start in trange(0, len(validation_labels), batch_size):\n",
    "    end = min(start+batch_size, len(validation_labels))\n",
    "    final_stats = sess.run(\n",
    "        [accuracy, clean_pred_cost, predictions],\n",
    "        feed_dict={feedforward_inputs: validation_instances[start:end],\n",
    "                   outputs: validation_labels[start:end],\n",
    "                   training: False})\n",
    "    mean_accuracy.append(final_stats[0])\n",
    "    mean_loss.append(final_stats[1])\n",
    "\n",
    "    true_labels = np.argmax(validation_labels[start:end], 1)\n",
    "    for i in np.arange(true_labels.shape[0]):\n",
    "        print(\"%s,validation,%d,%.3g,%.3g,%d,%d\" %\n",
    "              (config[\"experiment_id\"],\n",
    "               epoch_n,\n",
    "               final_stats[0],\n",
    "               final_stats[1],\n",
    "               true_labels[i],\n",
    "               final_stats[2][i]), file=results_log)\n",
    "\n",
    "print(\"Final Accuracy for Validation Data: %.3g\" % np.mean(mean_accuracy), file=sys.stderr)\n",
    "print(\"Final Supervised Cost for Validation Data: %.3g\" % np.mean(mean_loss), file=sys.stderr)\n",
    "\n",
    "### TEST DATA\n",
    "\n",
    "test_instances = data.test.instances\n",
    "test_labels = data.test.labels\n",
    "\n",
    "for start in trange(0, len(test_labels), batch_size):\n",
    "    end = min(start+batch_size, len(test_labels))\n",
    "    final_stats = sess.run(\n",
    "        [accuracy, clean_pred_cost, predictions],\n",
    "        feed_dict={feedforward_inputs: test_instances[start:end],\n",
    "                   outputs: test_labels[start:end],\n",
    "                   training: False})\n",
    "\n",
    "    true_labels = np.argmax(test_labels[start:end], 1)\n",
    "    for i in np.arange(true_labels.shape[0]):\n",
    "        print(\"%s,test,%d,%.3g,%.3g,%d,%d\" %\n",
    "              (config[\"experiment_id\"],\n",
    "               epoch_n,\n",
    "               final_stats[0],\n",
    "               final_stats[1],\n",
    "               true_labels[i],\n",
    "               final_stats[2][i]), file=results_log)\n",
    "\n",
    "print(\"=== Experiment finished ===\", file=sys.stderr)\n",
    "sess.close()\n",
    "results_log.close()\n",
    "\n",
    "return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
